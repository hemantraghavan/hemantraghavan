{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemantraghavan/hemantraghavan/blob/main/Hemant_MONAI_TotalSeg_Submitted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZUuoq6lym8D",
        "outputId": "d7aad2cf-2a62-4911-a350-eae79b1a0872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pathos 0.3.4 requires dill>=0.4.0, but you have dill 0.3.8 which is incompatible.\n",
            "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Running command git clone --filter=blob:none --quiet https://github.com/wasserth/TotalSegmentator.git /tmp/pip-req-build-fqwsekxi\n",
            "  Running command git checkout -q 23166611782f27756d140dce8abb5237715c2903\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.4.0 which is incompatible.\n",
            "datasets 4.0.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# âœ… STEP 1: Full clean install (dependency order matters!)\n",
        "!pip install --upgrade pip setuptools > /dev/null\n",
        "\n",
        "# Core dependencies\n",
        "!pip install numpy==1.26.4 scipy==1.11.4 pillow==10.3.0 matplotlib==3.9.0 > /dev/null\n",
        "\n",
        "# PyTorch (with CUDA 12.1 for T4 GPU)\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121 > /dev/null\n",
        "\n",
        "# Supporting libs\n",
        "!pip install dill==0.3.8 multiprocess==0.70.16 > /dev/null\n",
        "!pip install nibabel==5.1.0 scikit-image==0.19.3 pyvista==0.42.3 opencv-python-headless==4.9.0.80 > /dev/null\n",
        "\n",
        "# TotalSegmentator (after all dependencies)\n",
        "!pip install git+https://github.com/wasserth/TotalSegmentator.git@v2.1.0 > /dev/null\n",
        "\n",
        "# âœ… Finally: MONAI (install after numpy & torch to avoid setup issues)\n",
        "!pip install monai==1.3.0 > /dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugykLS7eB9TM",
        "outputId": "bcb3c347-e88b-451b-bbaa-20ee8bcce6f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/wasserth/TotalSegmentator.git /tmp/pip-req-build-hvnv9asb\n",
            "  Running command git checkout -q 23166611782f27756d140dce8abb5237715c2903\n"
          ]
        }
      ],
      "source": [
        "# I did a manual run-time reset for the Colab session, since the automated command was causing Colab session to crash.\n",
        "# Step 2: Re-installing Total Segmentator since it got cleared with the Run-time reset between step 1 & step 2.\n",
        "!pip install git+https://github.com/wasserth/TotalSegmentator.git@v2.1.0 > /dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlwQlOBsIWFQ",
        "outputId": "62ffd8ec-77cd-4cec-d8f1-d4da6c723d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/monai/utils/module.py:396: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  pkg = __import__(module)  # top level module\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All critical libraries imported successfully\n",
            "\n",
            "torch: 2.3.0+cu121\n",
            "monai: 1.3.0\n",
            "numpy: 1.26.4\n",
            "nibabel: 5.1.0\n",
            "scikit-image: 0.19.3\n",
            "pyvista: 0.42.3\n",
            "cv2: 4.11.0\n",
            "Version: 2.1.0\n"
          ]
        }
      ],
      "source": [
        "# Step 3 - Confirming that all critical libraries are imported successfully.\n",
        "# Confirmed that steps 1 thru 4 install all the correct libraries.\n",
        "import numpy\n",
        "import torch\n",
        "import monai\n",
        "import nibabel\n",
        "import skimage\n",
        "import pyvista\n",
        "import cv2\n",
        "import totalsegmentator  # âœ… Case-sensitive\n",
        "\n",
        "print(\"âœ… All critical libraries imported successfully\\n\")\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"monai:\", monai.__version__)\n",
        "print(\"numpy:\", numpy.__version__)\n",
        "print(\"nibabel:\", nibabel.__version__)\n",
        "print(\"scikit-image:\", skimage.__version__)\n",
        "print(\"pyvista:\", pyvista.__version__)\n",
        "print(\"cv2:\", cv2.__version__)\n",
        "# print(\"totalsegmentator:\", totalsegmentator.__version__)  # âŒ Not available\n",
        "!pip show TotalSegmentator | grep Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FYgtvU-IkjT",
        "outputId": "64b86649-abde-40b9-880f-6017408d2c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Step 4 - mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XpIanoX9Dq-"
      },
      "outputs": [],
      "source": [
        "# Step 5 - Re-installing critical libraries after run-time reset before Step 2.\n",
        "!pip install nibabel scikit-image trimesh pyvista matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6 - Segmentation using total segmentator\n",
        "# ðŸ”§ Install trimesh if needed\n",
        "!pip install -q trimesh\n",
        "\n",
        "# âœ… Imports\n",
        "import os\n",
        "from totalsegmentator.python_api import totalsegmentator\n",
        "import nibabel as nib\n",
        "from skimage import measure\n",
        "import numpy as np\n",
        "import trimesh\n",
        "import shutil\n",
        "\n",
        "# âœ… Set input/output paths\n",
        "dicom_input_path = \"/content/drive/MyDrive/Third_Case/Third_Case/DICOM/EXP00000_256\"\n",
        "output_folder = \"/content/drive/MyDrive/Third_Case/segmentation_output_free_heart\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# âœ… FINAL valid ROI subset for license-free task=\"total\"\n",
        "roi_subset = [\n",
        "    \"heart\",\n",
        "    \"aorta\",\n",
        "    \"rib_left_1\", \"rib_left_2\", \"rib_left_3\",\n",
        "    \"rib_right_1\", \"rib_right_2\", \"rib_right_3\",\n",
        "    \"spinal_cord\"\n",
        "]\n",
        "\n",
        "# âœ… Run inference using the pretrained free model\n",
        "totalsegmentator(\n",
        "    input=dicom_input_path,\n",
        "    output=output_folder,\n",
        "    task=\"total\",\n",
        "    roi_subset=roi_subset,\n",
        "    ml=True,\n",
        "    fast=False,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "SxQP-9YF_zVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d85579-3e4c-4274-a387-fbd3cdf9d5a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "If you use this tool please cite: https://pubs.rsna.org/doi/10.1148/ryai.230024\n",
            "\n",
            "Generating rough body segmentation...\n",
            "tmp_dir: /tmp/nnunet_tmp_391o97g6\n",
            "Converting dicom to nifti...\n",
            "  found image with shape (512, 512, 256)\n",
            "Resampling...\n",
            "  from shape (512, 512, 256) to shape (42, 42, 27)\n",
            "  Resampled in 2.86s\n",
            "Predicting...\n",
            "There are 1 cases in the source folder\n",
            "I am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
            "There are 1 cases that I would like to predict\n",
            "\n",
            "Predicting s01:\n",
            "perform_everything_on_device: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 92.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending off prediction to background worker for resampling and export\n",
            "done with s01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Predicted in 9.12s\n",
            "Resampling...\n",
            "  back to original shape: (512, 512, 256)\n",
            "Undoing canonical...\n",
            "Rough organ segmentation generated in 33.01s\n",
            "tmp_dir: /tmp/nnunet_tmp_svwhm02x\n",
            "Converting dicom to nifti...\n",
            "  found image with shape (512, 512, 256)\n",
            "  cropping from (512, 512, 256) to (512, 441, 256)\n",
            "Resampling...\n",
            "  from shape (512, 441, 256) to shape (167, 144, 107)\n",
            "  Resampled in 3.69s\n",
            "Computing parts: ['class_map_part_cardiac', 'class_map_part_muscles', 'class_map_part_ribs'] based on the provided roi_subset\n",
            "Predicting part 1 of 3 ...\n",
            "There are 1 cases in the source folder\n",
            "I am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
            "There are 1 cases that I would like to predict\n",
            "\n",
            "Predicting s01:\n",
            "perform_everything_on_device: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending off prediction to background worker for resampling and export\n",
            "done with s01\n",
            "Predicting part 2 of 3 ...\n",
            "There are 1 cases in the source folder\n",
            "I am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
            "There are 1 cases that I would like to predict\n",
            "\n",
            "Predicting s01:\n",
            "perform_everything_on_device: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 16.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending off prediction to background worker for resampling and export\n",
            "done with s01\n",
            "Predicting part 3 of 3 ...\n",
            "There are 1 cases in the source folder\n",
            "I am processing 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
            "There are 1 cases that I would like to predict\n",
            "\n",
            "Predicting s01:\n",
            "perform_everything_on_device: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 16.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending off prediction to background worker for resampling and export\n",
            "done with s01\n",
            "  Predicted in 35.24s\n",
            "Resampling...\n",
            "  back to original shape: (512, 441, 256)\n",
            "Undoing canonical...\n",
            "Undoing cropping...\n",
            "Saving segmentations...\n",
            "  Saved in 1.63s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nibabel.nifti1.Nifti1Image at 0x7b1f82e8f610>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-installing critical packages for exporting as STL.\n",
        "!pip install nibabel scikit-image trimesh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKEf99fAe0qu",
        "outputId": "b81e65f6-7d04-49c3-81c6-d187bd5b124d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.19.3)\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.11/dist-packages (4.7.1)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from nibabel) (1.26.4)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.11/dist-packages (from nibabel) (25.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.11.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (10.3.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.6.11)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7 - Exporting each section out as STL file\n",
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from skimage import measure\n",
        "import trimesh\n",
        "\n",
        "# âœ… Set path to segmentation result\n",
        "segmentation_path = \"/content/drive/MyDrive/Third_Case/segmentation_output_free_heart.nii\"\n",
        "output_stl_dir = \"/content/drive/MyDrive/Third_Case/segmentation_output_free_heart\"\n",
        "os.makedirs(output_stl_dir, exist_ok=True)\n",
        "\n",
        "# âœ… Load segmentation volume\n",
        "seg_img = nib.load(segmentation_path)\n",
        "seg_data = seg_img.get_fdata().astype(np.uint8)\n",
        "affine = seg_img.affine\n",
        "\n",
        "# âœ… Map structure labels from header if available\n",
        "label_map = seg_img.header.get(\"aux_file\", None)\n",
        "\n",
        "# âœ… Iterate through unique label values (excluding background 0)\n",
        "for label_value in np.unique(seg_data):\n",
        "    if label_value == 0:\n",
        "        continue  # skip background\n",
        "\n",
        "    # Create binary mask for current label\n",
        "    binary_mask = (seg_data == label_value).astype(np.uint8)\n",
        "\n",
        "    # Extract surface using marching cubes\n",
        "    verts, faces, _, _ = measure.marching_cubes(binary_mask, level=0.5)\n",
        "\n",
        "    # Convert to world coordinates using affine\n",
        "    verts = nib.affines.apply_affine(affine, verts)\n",
        "\n",
        "    # Create and export STL mesh\n",
        "    mesh = trimesh.Trimesh(vertices=verts, faces=faces)\n",
        "\n",
        "    stl_filename = f\"structure_{label_value}.stl\"\n",
        "    stl_path = os.path.join(output_stl_dir, stl_filename)\n",
        "    mesh.export(stl_path)\n",
        "\n",
        "    print(f\"âœ… Exported: {stl_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb9IGEcOU7UA",
        "outputId": "54522fd5-d775-4ce3-b5f1-8a393c06fe11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Exported: structure_51.stl\n",
            "âœ… Exported: structure_52.stl\n",
            "âœ… Exported: structure_79.stl\n",
            "âœ… Exported: structure_93.stl\n",
            "âœ… Exported: structure_95.stl\n",
            "âœ… Exported: structure_96.stl\n",
            "âœ… Exported: structure_107.stl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nibabel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRELYcR3mlSu",
        "outputId": "ac411cae-bc93-4b9c-e84d-95d07d2e2196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from nibabel) (1.26.4)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.11/dist-packages (from nibabel) (25.0)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN1LZbkBM03XVen5ytL+7LM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}